{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e249c612",
   "metadata": {},
   "source": [
    "# Projects in Math Modelling - Part 1 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f20d2f-1d79-4e27-b7c5-2cb303be22d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bc1de95-8ff7-41f2-901d-13cde0e628f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b05e2e-ddec-4643-b514-180206929ce5",
   "metadata": {},
   "source": [
    "## News API Lea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad6d0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes=pd.concat=('Fred Kerley', 'Noah Lyles', 'Akani Simbine' , 'Andre De Grasse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3279bb2",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33dcece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tweepy\n",
    "import json\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8622235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"twitter_key.json\") as infile:\n",
    "    json_obj = json.load(infile)\n",
    "    token =json_obj[\"bearer_token\"]\n",
    "    client = tweepy.Client(bearer_token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc71d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49c78248",
   "metadata": {},
   "source": [
    "### NewsAPI.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc994bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newsapi-python\n",
      "  Downloading newsapi_python-0.2.7-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from newsapi-python) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0->newsapi-python) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0->newsapi-python) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0->newsapi-python) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0->newsapi-python) (2024.2.2)\n",
      "Downloading newsapi_python-0.2.7-py2.py3-none-any.whl (7.9 kB)\n",
      "Installing collected packages: newsapi-python\n",
      "Successfully installed newsapi-python-0.2.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install newsapi-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2a44e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import pandas as pd\n",
    "\n",
    "# Init\n",
    "newsapi = NewsApiClient(api_key='2ffa9fd62f4e4dd8a0fc7697bd296069')\n",
    "\n",
    "# Dummy list to store all articles\n",
    "all_articles_list = []\n",
    "\n",
    "# Loop through each word and fetch articles\n",
    "for athlete in athletes:\n",
    "    for page in range(1, 6):  # Loop though pages\n",
    "        articles = newsapi.get_everything(q=athlete,\n",
    "                                          sort_by='relevancy',\n",
    "                                          page=page)\n",
    "        # Extract the articles and add a new column for the search term\n",
    "        if 'articles' in articles:\n",
    "            for article in articles['articles']:\n",
    "                article['athletes'] = athlete\n",
    "                all_articles_list.append(article)\n",
    "        else:\n",
    "            break  # Exit the loop if no more articles are returned\n",
    "\n",
    "# Convert the list of articles into a DataFrame\n",
    "df = pd.DataFrame(all_articles_list)\n",
    "\n",
    "encoded = json.dumps(all_articles_list)\n",
    "all_articles_list = json.loads(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86b60ef4-1aec-4273-8ae0-a52ea140898c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>urlToImage</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>content</th>\n",
       "      <th>athletes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'id': None, 'name': 'BBC News'}</td>\n",
       "      <td>BBC Sport</td>\n",
       "      <td>Asher-Smith wins 200m at Jamaica Athletics Inv...</td>\n",
       "      <td>Britain's Dina Asher-Smith wins gold in the 20...</td>\n",
       "      <td>https://www.bbc.com/sport/athletics/articles/c...</td>\n",
       "      <td>https://ichef.bbci.co.uk/news/1024/branded_spo...</td>\n",
       "      <td>2024-05-12T09:27:55Z</td>\n",
       "      <td>Britain's Dina Asher-Smith won gold in the 200...</td>\n",
       "      <td>Fred Kerley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'id': None, 'name': 'Forbes'}</td>\n",
       "      <td>Katelyn Hutchison, Contributor, \\n Katelyn Hut...</td>\n",
       "      <td>Kenny Bednarek Dominates Doha Diamond League A...</td>\n",
       "      <td>Kenny Bednarek shows why he has a strong chanc...</td>\n",
       "      <td>https://www.forbes.com/sites/katelynhutchison/...</td>\n",
       "      <td>https://imageio.forbes.com/specials-images/ima...</td>\n",
       "      <td>2024-05-12T02:58:33Z</td>\n",
       "      <td>DOHA, QATAR - MAY 10: Kenneth Bednarek of Team...</td>\n",
       "      <td>Fred Kerley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'id': 'bbc-news', 'name': 'BBC News'}</td>\n",
       "      <td>BBC Sport</td>\n",
       "      <td>Asher-Smith wins 200m at Jamaica Athletics Inv...</td>\n",
       "      <td>Britain's Dina Asher-Smith wins gold in the 20...</td>\n",
       "      <td>https://www.bbc.co.uk/sport/athletics/articles...</td>\n",
       "      <td>https://ichef.bbci.co.uk/news/1024/branded_spo...</td>\n",
       "      <td>2024-05-12T09:27:55Z</td>\n",
       "      <td>Britain's Dina Asher-Smith won gold in the 200...</td>\n",
       "      <td>Fred Kerley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'id': 'cbc-news', 'name': 'CBC News'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Lyles' record-setting run in Atlanta sets off ...</td>\n",
       "      <td>As CBC Sports senior contributor Morgan Campbe...</td>\n",
       "      <td>https://www.cbc.ca/sports/olympics/summer/trac...</td>\n",
       "      <td>https://i.cbc.ca/1.7211689.1716409143!/fileIma...</td>\n",
       "      <td>2024-05-23T08:00:00Z</td>\n",
       "      <td>Shout out to South African sprinter Akani Simb...</td>\n",
       "      <td>Fred Kerley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'id': 'cbc-news', 'name': 'CBC News'}</td>\n",
       "      <td>None</td>\n",
       "      <td>De Grasse wins 100m in Ostrava over training p...</td>\n",
       "      <td>Buoyed by a strong final 20 metres, Andre De G...</td>\n",
       "      <td>https://www.cbc.ca/sports/olympics/summer/trac...</td>\n",
       "      <td>https://i.cbc.ca/1.7217157.1716916196!/fileIma...</td>\n",
       "      <td>2024-05-28T17:21:31Z</td>\n",
       "      <td>Buoyed by a strong final 20 metres, Andre De G...</td>\n",
       "      <td>Fred Kerley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>{'id': 'ansa', 'name': 'ANSA.it'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Atletica: Diamond League; Bruni 2/a nell'asta ...</td>\n",
       "      <td>Bel secondo posto per Roberta Bruni a Marrakec...</td>\n",
       "      <td>https://www.ansa.it/sito/notizie/sport/altrisp...</td>\n",
       "      <td>https://www.ansa.it/webimages/img_700/2024/3/2...</td>\n",
       "      <td>2024-05-19T20:20:46Z</td>\n",
       "      <td>Bel secondo posto per Roberta Bruni a\\r\\nMarra...</td>\n",
       "      <td>Andre De Grasse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>{'id': 'ansa', 'name': 'ANSA.it'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Atletica: domani meeting Ostrava con Jacobs, F...</td>\n",
       "      <td>Otto azzurri in gara domani al Golden Spike di...</td>\n",
       "      <td>https://www.ansa.it/sito/notizie/sport/altrisp...</td>\n",
       "      <td>https://www.ansa.it/webimages/img_700/2024/5/1...</td>\n",
       "      <td>2024-05-27T16:52:10Z</td>\n",
       "      <td>Otto azzurri in gara domani al\\r\\nGolden Spike...</td>\n",
       "      <td>Andre De Grasse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>{'id': None, 'name': 'Eurosport.fr'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Bolt à l'infini : le 200m qui a cimenté sa lég...</td>\n",
       "      <td>Jeux olympiques - Après le triplé sur 100m, Us...</td>\n",
       "      <td>https://www.eurosport.fr/athletisme/rio/2016/j...</td>\n",
       "      <td>https://imgresizer.eurosport.com/unsafe/2560x1...</td>\n",
       "      <td>2024-05-17T22:10:03Z</td>\n",
       "      <td>Jeux olympiques - Après le triplé sur 100m, Us...</td>\n",
       "      <td>Andre De Grasse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>{'id': None, 'name': 'Iltalehti.fi'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Wilma Murrolle kova haastaja</td>\n",
       "      <td>Turussa nähdään todellista yleisurheilun tähti...</td>\n",
       "      <td>https://www.iltalehti.fi/yleisurheilu/a/a0b9cc...</td>\n",
       "      <td>https://img.ilcdn.fi/deoDK-eFuesfLqw620SwzWT8h...</td>\n",
       "      <td>2024-06-05T08:09:39Z</td>\n",
       "      <td>Turun Paavo Nurmi Games -kilpailuissa nähdään ...</td>\n",
       "      <td>Andre De Grasse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>{'id': None, 'name': 'Iltalehti.fi'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Valtavia maailmantähtiä Suomen Turkuun</td>\n",
       "      <td>Paavo Nurmi Games kiinnitti kaksi olympiavoitt...</td>\n",
       "      <td>https://www.iltalehti.fi/yleisurheilu/a/afcd74...</td>\n",
       "      <td>https://img.ilcdn.fi/OXNb4KA_QF-ur2pJ91D9BOyD8...</td>\n",
       "      <td>2024-06-04T17:35:18Z</td>\n",
       "      <td>Suomen Turussa on tänäkin vuonna kovat piipuss...</td>\n",
       "      <td>Andre De Grasse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     source  \\\n",
       "0          {'id': None, 'name': 'BBC News'}   \n",
       "1            {'id': None, 'name': 'Forbes'}   \n",
       "2    {'id': 'bbc-news', 'name': 'BBC News'}   \n",
       "3    {'id': 'cbc-news', 'name': 'CBC News'}   \n",
       "4    {'id': 'cbc-news', 'name': 'CBC News'}   \n",
       "..                                      ...   \n",
       "98        {'id': 'ansa', 'name': 'ANSA.it'}   \n",
       "99        {'id': 'ansa', 'name': 'ANSA.it'}   \n",
       "100    {'id': None, 'name': 'Eurosport.fr'}   \n",
       "101    {'id': None, 'name': 'Iltalehti.fi'}   \n",
       "102    {'id': None, 'name': 'Iltalehti.fi'}   \n",
       "\n",
       "                                                author  \\\n",
       "0                                            BBC Sport   \n",
       "1    Katelyn Hutchison, Contributor, \\n Katelyn Hut...   \n",
       "2                                            BBC Sport   \n",
       "3                                                 None   \n",
       "4                                                 None   \n",
       "..                                                 ...   \n",
       "98                                                None   \n",
       "99                                                None   \n",
       "100                                               None   \n",
       "101                                               None   \n",
       "102                                               None   \n",
       "\n",
       "                                                 title  \\\n",
       "0    Asher-Smith wins 200m at Jamaica Athletics Inv...   \n",
       "1    Kenny Bednarek Dominates Doha Diamond League A...   \n",
       "2    Asher-Smith wins 200m at Jamaica Athletics Inv...   \n",
       "3    Lyles' record-setting run in Atlanta sets off ...   \n",
       "4    De Grasse wins 100m in Ostrava over training p...   \n",
       "..                                                 ...   \n",
       "98   Atletica: Diamond League; Bruni 2/a nell'asta ...   \n",
       "99   Atletica: domani meeting Ostrava con Jacobs, F...   \n",
       "100  Bolt à l'infini : le 200m qui a cimenté sa lég...   \n",
       "101                       Wilma Murrolle kova haastaja   \n",
       "102             Valtavia maailmantähtiä Suomen Turkuun   \n",
       "\n",
       "                                           description  \\\n",
       "0    Britain's Dina Asher-Smith wins gold in the 20...   \n",
       "1    Kenny Bednarek shows why he has a strong chanc...   \n",
       "2    Britain's Dina Asher-Smith wins gold in the 20...   \n",
       "3    As CBC Sports senior contributor Morgan Campbe...   \n",
       "4    Buoyed by a strong final 20 metres, Andre De G...   \n",
       "..                                                 ...   \n",
       "98   Bel secondo posto per Roberta Bruni a Marrakec...   \n",
       "99   Otto azzurri in gara domani al Golden Spike di...   \n",
       "100  Jeux olympiques - Après le triplé sur 100m, Us...   \n",
       "101  Turussa nähdään todellista yleisurheilun tähti...   \n",
       "102  Paavo Nurmi Games kiinnitti kaksi olympiavoitt...   \n",
       "\n",
       "                                                   url  \\\n",
       "0    https://www.bbc.com/sport/athletics/articles/c...   \n",
       "1    https://www.forbes.com/sites/katelynhutchison/...   \n",
       "2    https://www.bbc.co.uk/sport/athletics/articles...   \n",
       "3    https://www.cbc.ca/sports/olympics/summer/trac...   \n",
       "4    https://www.cbc.ca/sports/olympics/summer/trac...   \n",
       "..                                                 ...   \n",
       "98   https://www.ansa.it/sito/notizie/sport/altrisp...   \n",
       "99   https://www.ansa.it/sito/notizie/sport/altrisp...   \n",
       "100  https://www.eurosport.fr/athletisme/rio/2016/j...   \n",
       "101  https://www.iltalehti.fi/yleisurheilu/a/a0b9cc...   \n",
       "102  https://www.iltalehti.fi/yleisurheilu/a/afcd74...   \n",
       "\n",
       "                                            urlToImage           publishedAt  \\\n",
       "0    https://ichef.bbci.co.uk/news/1024/branded_spo...  2024-05-12T09:27:55Z   \n",
       "1    https://imageio.forbes.com/specials-images/ima...  2024-05-12T02:58:33Z   \n",
       "2    https://ichef.bbci.co.uk/news/1024/branded_spo...  2024-05-12T09:27:55Z   \n",
       "3    https://i.cbc.ca/1.7211689.1716409143!/fileIma...  2024-05-23T08:00:00Z   \n",
       "4    https://i.cbc.ca/1.7217157.1716916196!/fileIma...  2024-05-28T17:21:31Z   \n",
       "..                                                 ...                   ...   \n",
       "98   https://www.ansa.it/webimages/img_700/2024/3/2...  2024-05-19T20:20:46Z   \n",
       "99   https://www.ansa.it/webimages/img_700/2024/5/1...  2024-05-27T16:52:10Z   \n",
       "100  https://imgresizer.eurosport.com/unsafe/2560x1...  2024-05-17T22:10:03Z   \n",
       "101  https://img.ilcdn.fi/deoDK-eFuesfLqw620SwzWT8h...  2024-06-05T08:09:39Z   \n",
       "102  https://img.ilcdn.fi/OXNb4KA_QF-ur2pJ91D9BOyD8...  2024-06-04T17:35:18Z   \n",
       "\n",
       "                                               content         athletes  \n",
       "0    Britain's Dina Asher-Smith won gold in the 200...      Fred Kerley  \n",
       "1    DOHA, QATAR - MAY 10: Kenneth Bednarek of Team...      Fred Kerley  \n",
       "2    Britain's Dina Asher-Smith won gold in the 200...      Fred Kerley  \n",
       "3    Shout out to South African sprinter Akani Simb...      Fred Kerley  \n",
       "4    Buoyed by a strong final 20 metres, Andre De G...      Fred Kerley  \n",
       "..                                                 ...              ...  \n",
       "98   Bel secondo posto per Roberta Bruni a\\r\\nMarra...  Andre De Grasse  \n",
       "99   Otto azzurri in gara domani al\\r\\nGolden Spike...  Andre De Grasse  \n",
       "100  Jeux olympiques - Après le triplé sur 100m, Us...  Andre De Grasse  \n",
       "101  Turun Paavo Nurmi Games -kilpailuissa nähdään ...  Andre De Grasse  \n",
       "102  Suomen Turussa on tänäkin vuonna kovat piipuss...  Andre De Grasse  \n",
       "\n",
       "[103 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caeec06",
   "metadata": {},
   "source": [
    "### Google News API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf3ac2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import requests\n",
    "\n",
    "API_KEY = os.environ.get(\"API_KEY\")\n",
    "# set the api key in headers\n",
    "headers = {\"apikey\": API_KEY}\n",
    "\n",
    "# format the query\n",
    "# q: the search term\n",
    "query = {\"q\": \"Fred Kerley\"}\n",
    "\n",
    "# build to url to make request\n",
    "url = f\"https://api.serply.io/v1/news/\" + urllib.parse.urlencode(query)\n",
    "\n",
    "resp = requests.get(url, headers=headers)\n",
    "results = resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb360a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch results for Fred Kerley: 401\n",
      "Failed to fetch results for Noah Lyles: 401\n",
      "Failed to fetch results for Akani Simbine: 401\n",
      "Failed to fetch results for Andre De Grasse: 401\n"
     ]
    }
   ],
   "source": [
    "#401 =not found\n",
    "import os\n",
    "import urllib.parse\n",
    "import requests\n",
    "\n",
    "# Replace with your actual API key\n",
    "API_KEY = os.environ.get(\"API_KEY\")\n",
    "# Set the API key in headers\n",
    "headers = {\"apikey\": API_KEY}\n",
    "\n",
    "# Loop through each athlete and fetch news articles\n",
    "for athlete in athletes:\n",
    "    # Format the query\n",
    "    query = {\"q\": athlete}\n",
    "    \n",
    "    # Build the URL to make the request\n",
    "    url = f\"https://api.serply.io/v1/news?\" + urllib.parse.urlencode(query)\n",
    "    \n",
    "    # Make the request\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if resp.status_code == 200:\n",
    "        results = resp.json()\n",
    "        \n",
    "        # Print the results\n",
    "        print(f\"Results for {athlete}:\")\n",
    "        for article in results.get('articles', []):\n",
    "            print(\"Title:\", article['title'])\n",
    "            print(\"Description:\", article['description'])\n",
    "            print(\"URL:\", article['url'])\n",
    "            print()\n",
    "    else:\n",
    "        print(f\"Failed to fetch results for {athlete}: {resp.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62fd0dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>urlToImage</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>content</th>\n",
       "      <th>athletes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'id': None, 'name': 'BBC News'}</td>\n",
       "      <td>BBC Sport</td>\n",
       "      <td>Asher-Smith wins 200m at Jamaica Athletics Inv...</td>\n",
       "      <td>Britain's Dina Asher-Smith wins gold in the 20...</td>\n",
       "      <td>https://www.bbc.com/sport/athletics/articles/c...</td>\n",
       "      <td>https://ichef.bbci.co.uk/news/1024/branded_spo...</td>\n",
       "      <td>2024-05-12T09:27:55Z</td>\n",
       "      <td>Britain's Dina Asher-Smith won gold in the 200...</td>\n",
       "      <td>Fred Kerley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'id': None, 'name': 'Forbes'}</td>\n",
       "      <td>Paras Jan, Contributor, \\n Paras Jan, Contribu...</td>\n",
       "      <td>Sha’Carri Richardson Faces Yet Another Loss At...</td>\n",
       "      <td>Following the loss in Xiamen League, in the 20...</td>\n",
       "      <td>https://www.forbes.com/sites/parasjan/2024/04/...</td>\n",
       "      <td>https://imageio.forbes.com/specials-images/ima...</td>\n",
       "      <td>2024-04-28T09:24:42Z</td>\n",
       "      <td>EUGENE, OREGON - MAY 28: Sha'Carri Richardson ...</td>\n",
       "      <td>Fred Kerley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'id': None, 'name': 'Minneapolis Star Tribune'}</td>\n",
       "      <td>Star Tribune Staff</td>\n",
       "      <td>Simbine upstages Coleman and Kerley to win 100...</td>\n",
       "      <td>Akani Simbine surged late to win the 100 meter...</td>\n",
       "      <td>https://www.startribune.com/simbine-upstages-c...</td>\n",
       "      <td>https://www.startribune.com/static/img/brandin...</td>\n",
       "      <td>2024-04-27T14:21:01Z</td>\n",
       "      <td>SUZHOU, China Akani Simbine surged late to win...</td>\n",
       "      <td>Fred Kerley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'id': None, 'name': 'Forbes'}</td>\n",
       "      <td>Katelyn Hutchison, Contributor, \\n Katelyn Hut...</td>\n",
       "      <td>Kenny Bednarek Dominates Doha Diamond League A...</td>\n",
       "      <td>Kenny Bednarek shows why he has a strong chanc...</td>\n",
       "      <td>https://www.forbes.com/sites/katelynhutchison/...</td>\n",
       "      <td>https://imageio.forbes.com/specials-images/ima...</td>\n",
       "      <td>2024-05-12T02:58:33Z</td>\n",
       "      <td>DOHA, QATAR - MAY 10: Kenneth Bednarek of Team...</td>\n",
       "      <td>Fred Kerley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'id': None, 'name': 'CNA'}</td>\n",
       "      <td>None</td>\n",
       "      <td>American Coleman believes Bolt's 100m record c...</td>\n",
       "      <td>SHANGHAI : American sprinter Christian Coleman...</td>\n",
       "      <td>https://www.channelnewsasia.com/sport/american...</td>\n",
       "      <td>https://onecms-res.cloudinary.com/image/upload...</td>\n",
       "      <td>2024-04-26T09:46:52Z</td>\n",
       "      <td>SHANGHAI : American sprinter Christian Coleman...</td>\n",
       "      <td>Fred Kerley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             source  \\\n",
       "0                  {'id': None, 'name': 'BBC News'}   \n",
       "1                    {'id': None, 'name': 'Forbes'}   \n",
       "2  {'id': None, 'name': 'Minneapolis Star Tribune'}   \n",
       "3                    {'id': None, 'name': 'Forbes'}   \n",
       "4                       {'id': None, 'name': 'CNA'}   \n",
       "\n",
       "                                              author  \\\n",
       "0                                          BBC Sport   \n",
       "1  Paras Jan, Contributor, \\n Paras Jan, Contribu...   \n",
       "2                                 Star Tribune Staff   \n",
       "3  Katelyn Hutchison, Contributor, \\n Katelyn Hut...   \n",
       "4                                               None   \n",
       "\n",
       "                                               title  \\\n",
       "0  Asher-Smith wins 200m at Jamaica Athletics Inv...   \n",
       "1  Sha’Carri Richardson Faces Yet Another Loss At...   \n",
       "2  Simbine upstages Coleman and Kerley to win 100...   \n",
       "3  Kenny Bednarek Dominates Doha Diamond League A...   \n",
       "4  American Coleman believes Bolt's 100m record c...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Britain's Dina Asher-Smith wins gold in the 20...   \n",
       "1  Following the loss in Xiamen League, in the 20...   \n",
       "2  Akani Simbine surged late to win the 100 meter...   \n",
       "3  Kenny Bednarek shows why he has a strong chanc...   \n",
       "4  SHANGHAI : American sprinter Christian Coleman...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.bbc.com/sport/athletics/articles/c...   \n",
       "1  https://www.forbes.com/sites/parasjan/2024/04/...   \n",
       "2  https://www.startribune.com/simbine-upstages-c...   \n",
       "3  https://www.forbes.com/sites/katelynhutchison/...   \n",
       "4  https://www.channelnewsasia.com/sport/american...   \n",
       "\n",
       "                                          urlToImage           publishedAt  \\\n",
       "0  https://ichef.bbci.co.uk/news/1024/branded_spo...  2024-05-12T09:27:55Z   \n",
       "1  https://imageio.forbes.com/specials-images/ima...  2024-04-28T09:24:42Z   \n",
       "2  https://www.startribune.com/static/img/brandin...  2024-04-27T14:21:01Z   \n",
       "3  https://imageio.forbes.com/specials-images/ima...  2024-05-12T02:58:33Z   \n",
       "4  https://onecms-res.cloudinary.com/image/upload...  2024-04-26T09:46:52Z   \n",
       "\n",
       "                                             content     athletes  \n",
       "0  Britain's Dina Asher-Smith won gold in the 200...  Fred Kerley  \n",
       "1  EUGENE, OREGON - MAY 28: Sha'Carri Richardson ...  Fred Kerley  \n",
       "2  SUZHOU, China Akani Simbine surged late to win...  Fred Kerley  \n",
       "3  DOHA, QATAR - MAY 10: Kenneth Bednarek of Team...  Fred Kerley  \n",
       "4  SHANGHAI : American sprinter Christian Coleman...  Fred Kerley  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5aac2168-bacc-40cf-8ae6-e7beb18a93c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Articles Database json Agathe\n",
    "\n",
    "I didn't know what the articles were about, it seems that it is about the NFL... I still need to perform some more analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18457d03-54fb-458a-8c63-d4c75b0ab974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Text: > Pennsylvania Bell's 1-yard TD run at buzzer lifts Steelers over Chargers Pittsburgh Steelers head coach Mike Tomlin celebrates defeating the San Diego Chargers in an NFL football game Monday, Oct. 12, 2015, in San Diego. The Steelers won 24-20. (AP Photo/Gregory Bull) \n",
      "© 2015 Cox Media Group . By using this website, you accept the terms of our Visitor Agreement and Privacy Policy , and understand your options regarding Ad Choices . \n",
      "Need a Profile? Register Now. {* #userInformationForm *} Forgot your password? {* traditionalSignIn_password *} {* traditionalSignIn_signInButton *} {* /userInformationForm *} Sign in using your existing account {* loginWidget *} \n",
      "Need a Profile? Register Now. {* #userInformationForm *} Forgot your password? {* traditionalSignIn_password *} {* traditionalSignIn_signInButton *} {* /userInformationForm *} Sign in using your existing account {* loginWidget *} \n",
      "Need a Profile? Register Now. {* #userInformationForm *} Forgot your password? {* traditionalSignIn_password *} {* traditionalSignIn_signInButton *} {* /userInformationForm *} Sign in using your existing account {* loginWidget *} Please confirm the information below before signing in. *Required By submitting your registration information, you agree to our Visitor Agreement and Privacy Policy . {* socialRegistration_signInButton *} {* /socialRegistrationForm *} {* #registrationForm *} {* traditionalRegistration_emailAddress *} {* traditionalRegistration_password *} {* traditionalRegistration_passwordConfirm *} {* traditionalRegistration_displayName *} By submitting your registration information, you agree to our Visitor Agreement and Privacy Policy . {* createAccountButton *} {* /registrationForm *} {* #registrationFormBlank *} {* traditionalRegistration_emailAddressBlank *} {* traditionalRegistration_passwordBlank *} {* traditionalRegistration_passwordConfirmBlank *} {* traditionalRegistration_displayName *} By submitting your registration information, you agree to our Visitor Agreement and Privacy Policy . {* createAccountButton *} {* /registrationForm *} Just One More Thing... \n",
      "We have sent you a verification email. Please check your email and click on the link to activate your profile. \n",
      "If you do not receive the verification message within a few minutes of signing up, please check your Spam or Junk folder. Close Thank you for registering! \n",
      "We look forward to seeing you on [website] frequently. Visit us and sign in to update your profile, receive the latest news and keep up to date with mobile alerts. \n",
      "Click here to return to the page you were visiting. Create a new password \n",
      "Don't worry, it happens. We'll send you a link to create a new password. {* #forgotPasswordForm *} {* forgotPassword_emailAddress *} {* forgotPassword_sendButton *} {* /forgotPasswordForm *}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the path to the JSON file\n",
    "file_path = 'data-articles/news_0000002.json'\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extracting the necessary details from the JSON data\n",
    "article_text = data[\"text\"]\n",
    "\n",
    "# Print the extracted details\n",
    "print(\"Article Text:\", article_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69521799-8691-4bb3-9931-b333e6892527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 1 Text: > South Carolina Spurrier resigns, takes blame for South Carolina's slide South Carolina head football coach Steve Spurrier speaks at a news conference to announce he is resigning on Tuesday, Oct. 13, 2015, at the University Of South Carolina, in Columbia, S.C. Spurrier said he felt he needed to step down now because he doesn't believe there is accountability with players if they know the coach won't be back next year. He also said he was a recruiting liability. He had never had a losing season in 25 previous seasons coach at Duke (1987-89), Florida (1990-2001) or South Carolina, where he has been since 2005. (AP Photo/Richard Shiro)\n",
      "------------------------------------------------------------\n",
      "Article 2 Text: > Pennsylvania Bell's 1-yard TD run at buzzer lifts Steelers over Chargers Pittsburgh Steelers head coach Mike Tomlin celebrates defeating the San Diego Chargers in an NFL football game Monday, Oct. 12, 2015, in San Diego. The Steelers won 24-20. (AP Photo/Gregory Bull) \n",
      "Â© 2015 Cox Media Group . By using this website, you accept the terms of our Visitor Agreement and Privacy Policy , and understand your options regarding Ad Choices . \n",
      "Need a Profile? Register Now. {* #userInformationForm *} Forgot your password? {* traditionalSignIn_password *} {* traditionalSignIn_signInButton *} {* /userInformationForm *} Sign in using your existing account {* loginWidget *} \n",
      "Need a Profile? Register Now. {* #userInformationForm *} Forgot your password? {* traditionalSignIn_password *} {* traditionalSignIn_signInButton *} {* /userInformationForm *} Sign in using your existing account {* loginWidget *} \n",
      "Need a Profile? Register Now. {* #userInformationForm *} Forgot your password? {* traditionalSignIn_password *} {* traditionalSignIn_signInButton *} {* /userInformationForm *} Sign in using your existing account {* loginWidget *} Please confirm the information below before signing in. *Required By submitting your registration information, you agree to our Visitor Agreement and Privacy Policy . {* socialRegistration_signInButton *} {* /socialRegistrationForm *} {* #registrationForm *} {* traditionalRegistration_emailAddress *} {* traditionalRegistration_password *} {* traditionalRegistration_passwordConfirm *} {* traditionalRegistration_displayName *} By submitting your registration information, you agree to our Visitor Agreement and Privacy Policy . {* createAccountButton *} {* /registrationForm *} {* #registrationFormBlank *} {* traditionalRegistration_emailAddressBlank *} {* traditionalRegistration_passwordBlank *} {* traditionalRegistration_passwordConfirmBlank *} {* traditionalRegistration_displayName *} By submitting your registration information, you agree to our Visitor Agreement and Privacy Policy . {* createAccountButton *} {* /registrationForm *} Just One More Thing... \n",
      "We have sent you a verification email. Please check your email and click on the link to activate your profile. \n",
      "If you do not receive the verification message within a few minutes of signing up, please check your Spam or Junk folder. Close Thank you for registering! \n",
      "We look forward to seeing you on [website] frequently. Visit us and sign in to update your profile, receive the latest news and keep up to date with mobile alerts. \n",
      "Click here to return to the page you were visiting. Create a new password \n",
      "Don't worry, it happens. We'll send you a link to create a new password. {* #forgotPasswordForm *} {* forgotPassword_emailAddress *} {* forgotPassword_sendButton *} {* /forgotPasswordForm *}\n",
      "------------------------------------------------------------\n",
      "Article 3 Text: Published By: Finger Lake Times - Today \n",
      "NEW YORK (AP) â€” Jail watchdogs have expressed skepticism over a plan to mandate all New York City inmates wear uniforms....\n",
      "------------------------------------------------------------\n",
      "Article 4 Text: Published By: Charlotte Observer: Crime - Today \n",
      "Man confronted people who didnâ€™t belong at houseHe copied down their license plate numberMount Holly woman arrested and charged â€¦ Click ...\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 2149: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d653df8f0a12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# Open and read the JSON file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# Extracting the necessary details from the JSON data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m     \"\"\"\n\u001b[1;32m--> 293\u001b[1;33m     return loads(fp.read(),\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 2149: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Loop through files from 1 to 100\n",
    "for i in range(1, 101):\n",
    "    # Generate the file name with leading zeros\n",
    "    file_path = f'data-articles/news_{i:07d}.json'\n",
    "    \n",
    "    try:\n",
    "        # Open and read the JSON file\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Extracting the necessary details from the JSON data\n",
    "        article_text = data[\"text\"]\n",
    "        \n",
    "        # Print the extracted details\n",
    "        print(f\"Article {i} Text:\", article_text)\n",
    "        print(\"------------------------------------------------------------\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON in file {file_path}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57c71ab-86fa-431b-8c33-a182710fd9b7",
   "metadata": {},
   "source": [
    "Let's see what sports these articles are about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e8a204c-c9ce-4e50-b102-29405afcd4d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-37e7f17a23c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Directory to save the filtered articles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0moutput_directory_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'data-articles/filtered-articles'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_directory_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Loop through files from 1 to 100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "## Highlight the sport of each article - put it it the same of the json file\n",
    "\n",
    "# Define the sports keywords to look for\n",
    "sports_keywords = [\"athletic\", \"NFL\", \"baseball\"]\n",
    "\n",
    "# Directory containing the JSON files\n",
    "input_directory_path = 'data-articles'\n",
    "\n",
    "# Directory to save the filtered articles\n",
    "output_directory_path = 'data-articles/filtered-articles'\n",
    "os.makedirs(output_directory_path, exist_ok=True)\n",
    "\n",
    "# Loop through files from 1 to 100\n",
    "for i in range(1, 101):\n",
    "    # Generate the input file name with leading zeros\n",
    "    input_file_path = os.path.join(input_directory_path, f'news_{i:07d}.json')\n",
    "    \n",
    "    try:\n",
    "        # Open and read the JSON file\n",
    "        with open(input_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Extract the text of the article\n",
    "        article_text = data.get(\"text\", \"\")\n",
    "        \n",
    "        # Check if any of the sports keywords are in the article text\n",
    "        found_keywords = [keyword for keyword in sports_keywords if keyword.lower() in article_text.lower()]\n",
    "        if found_keywords:\n",
    "            # Print the found keywords\n",
    "            print(f\"Article {i} mentions the following sports keywords: {', '.join(found_keywords)}\")\n",
    "            \n",
    "            # Save only the text to a new JSON file in the output directory\n",
    "            output_file_path = os.path.join(output_directory_path, f'filtered_{i:07d}_{found_keywords[0]}.json')\n",
    "            with open(output_file_path, 'w') as output_file:\n",
    "                json.dump({\"text\": article_text}, output_file, indent=4)\n",
    "            print(f\"Article {i} text has been saved to {output_file_path}.\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {input_file_path} not found.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON in file {input_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c4fa5-1327-4bd6-bb80-12b65f65ad39",
   "metadata": {},
   "source": [
    "So as you can see, we have mostly baseball and NFL articles....\n",
    "\n",
    "Let's find something else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e6fa2e-c078-4b0c-a729-ad0b60a133f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Get articles from google - Agathe\n",
    "Webscraping (not API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "998acce1-3b6b-4cd1-8d5d-211ac00ab371",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googlesearch-pythonNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading googlesearch_python-1.2.4-py3-none-any.whl (4.5 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\leaka\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\leaka\\anaconda3\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\leaka\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.2.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\leaka\\anaconda3\\lib\\site-packages (from requests) (1.26.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\leaka\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\leaka\\anaconda3\\lib\\site-packages (from requests) (3.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\leaka\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Installing collected packages: googlesearch-python\n",
      "Successfully installed googlesearch-python-1.2.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install googlesearch-python requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88f8f314-5d69-4177-9158-493e6424153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2d98f68-e73f-41cc-945d-a96bfafbbc89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Wikipedia articles for: athletics performance\n",
      "Fetching Wikipedia articles for: gymnastics\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-75e797f13ba8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mqueries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Fetching Wikipedia articles for: {query}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0marticles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_wikipedia_articles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[0mall_articles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-75e797f13ba8>\u001b[0m in \u001b[0;36mfetch_wikipedia_articles\u001b[1;34m(query, num_results)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;31m# Parse the HTML content\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;31m# Extract the title\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m                 \u001b[0msuccess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    432\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m         \u001b[1;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\builder\\_htmlparser.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m             \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m             \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mHTMLParseError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\html\\parser.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgoahead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\html\\parser.py\u001b[0m in \u001b[0;36mgoahead\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m    171\u001b[0m                     \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_starttag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"</\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                     \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_endtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"<!--\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m                     \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_comment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Find articles from a google search - RANDOM SPORTS article for nom\n",
    "\n",
    "def fetch_wikipedia_articles(query, num_results=10):\n",
    "    # Perform Google search limited to Wikipedia\n",
    "    search_results = search(query + \" site:en.wikipedia.org\", num_results=num_results, lang=\"en\")\n",
    "    \n",
    "    articles = []\n",
    "\n",
    "    for url in search_results:\n",
    "        try:\n",
    "            # Make a request to the URL\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extract the title\n",
    "            title = soup.find('title').text\n",
    "            \n",
    "            # Extract the content\n",
    "            content = ''\n",
    "            for paragraph in soup.find_all('p'):\n",
    "                content += paragraph.text + '\\n'\n",
    "            \n",
    "            # Save the article\n",
    "            articles.append({\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'content': content\n",
    "            })\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to fetch {url}: {e}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def save_articles(articles, folder_name):\n",
    "    # Create folder if it doesn't exist\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    \n",
    "    for idx, article in enumerate(articles, start=1):\n",
    "        filename = os.path.join(folder_name, f\"article_{idx}.txt\")\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(f\"Title: {article['title']}\\n\")\n",
    "            file.write(f\"URL: {article['url']}\\n\")\n",
    "            file.write(\"Content:\\n\")\n",
    "            file.write(article['content'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example queries\n",
    "    queries = [\"athletics performance\", \"gymnastics\"]\n",
    "    \n",
    "    all_articles = {}\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"Fetching Wikipedia articles for: {query}\")\n",
    "        articles = fetch_wikipedia_articles(query)\n",
    "        all_articles[query] = articles\n",
    "        \n",
    "    for query, articles in all_articles.items():\n",
    "        print(f\"\\nWikipedia Articles for query: {query}\")\n",
    "        save_articles(articles, \"google-api-data\")\n",
    "        print(\"Articles saved.\")\n",
    "\n",
    "### BE CAREFUL\n",
    "# You can get an error if you run too often this cell!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda237d9-8977-447f-aa3b-70a612e9b977",
   "metadata": {},
   "source": [
    "Now we want to get the articles of athletes. Let's first look at the 5000m 10 best athletes.\n",
    "Yomif KEJELCHA, Hagos GEBRHIWET, Berihu AREGAWI, Telahun Haile BEKELE, Jakob INGEBRIGTSEN, Jacob KIPLIMO, Selemon BAREGA,Grant FISHER,Luis GRIJALVA,Joshua CHEPTEGEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb2a6bab-c9ec-41bf-9daf-6f637d9a0651",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles for: Yomif KEJELCHA\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://www.google.com/sorry/index?continue=https://www.google.com/search%3Fq%3DYomif%252BKEJELCHA%252B5000m%252Bathlete%26num%3D12%26hl%3Den%26start%3D0%26safe%3Dactive&hl=en&q=EhAqAQ4KBbi-EIB2YOdWsrPtGKXZm7MGIjBVz46srs8fT7YoJvBZ1YuaZPgNzTbs2vHLIuTx1UflzQnI51bSu6ViyuSUMfD6p2AyAXJaAUM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m athlete \u001b[38;5;129;01min\u001b[39;00m athletes:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching articles for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mathlete\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m     articles \u001b[38;5;241m=\u001b[39m fetch_articles_for_athlete(athlete, num_results\u001b[38;5;241m=\u001b[39mnum_articles_per_athlete)\n\u001b[1;32m     72\u001b[0m     save_articles_for_athlete(athlete, articles, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle-api-data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArticles for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mathlete\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mfetch_articles_for_athlete\u001b[0;34m(athlete_name, num_results)\u001b[0m\n\u001b[1;32m      8\u001b[0m search_results \u001b[38;5;241m=\u001b[39m search(query, num_results\u001b[38;5;241m=\u001b[39mnum_results, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m articles \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m search_results:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# Make a request to the URL\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/googlesearch/__init__.py:57\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(term, num_results, lang, proxy, advanced, sleep_interval, timeout, safe, ssl_verify)\u001b[0m\n\u001b[1;32m     54\u001b[0m start \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m start \u001b[38;5;241m<\u001b[39m num_results:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Send request\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     resp \u001b[38;5;241m=\u001b[39m _req(escaped_term, num_results \u001b[38;5;241m-\u001b[39m start,\n\u001b[1;32m     58\u001b[0m                 lang, start, proxies, timeout, safe, ssl_verify)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Parse\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(resp\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/googlesearch/__init__.py:26\u001b[0m, in \u001b[0;36m_req\u001b[0;34m(term, results, lang, start, proxies, timeout, safe, ssl_verify)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_req\u001b[39m(term, results, lang, start, proxies, timeout, safe, ssl_verify):\n\u001b[1;32m     10\u001b[0m     resp \u001b[38;5;241m=\u001b[39m get(\n\u001b[1;32m     11\u001b[0m         url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.google.com/search\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m         headers\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m         verify\u001b[38;5;241m=\u001b[39mssl_verify,\n\u001b[1;32m     25\u001b[0m     )\n\u001b[0;32m---> 26\u001b[0m     resp\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://www.google.com/sorry/index?continue=https://www.google.com/search%3Fq%3DYomif%252BKEJELCHA%252B5000m%252Bathlete%26num%3D12%26hl%3Den%26start%3D0%26safe%3Dactive&hl=en&q=EhAqAQ4KBbi-EIB2YOdWsrPtGKXZm7MGIjBVz46srs8fT7YoJvBZ1YuaZPgNzTbs2vHLIuTx1UflzQnI51bSu6ViyuSUMfD6p2AyAXJaAUM"
     ]
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def fetch_articles_for_athlete(athlete_name, num_results=10):\n",
    "    query = f\"{athlete_name} 5000m athlete\"\n",
    "    search_results = search(query, num_results=num_results, lang=\"en\")\n",
    "\n",
    "    articles = []\n",
    "\n",
    "    for url in search_results:\n",
    "        try:\n",
    "            # Make a request to the URL\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract the title\n",
    "            title = soup.find('title').text\n",
    "\n",
    "            # Extract the content\n",
    "            content = ''\n",
    "            for paragraph in soup.find_all('p'):\n",
    "                content += paragraph.text + '\\n'\n",
    "\n",
    "            # Save the article\n",
    "            articles.append({\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'content': content\n",
    "            })\n",
    "\n",
    "        except requests.HTTPError as e:\n",
    "            print(f\"Failed to fetch {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "    return articles\n",
    "\n",
    "def save_articles_for_athlete(athlete_name, articles, folder_name):\n",
    "    # Create a folder for the athlete if it doesn't exist\n",
    "    athlete_folder = os.path.join(folder_name, athlete_name.replace(\" \", \"_\"))\n",
    "    if not os.path.exists(athlete_folder):\n",
    "        os.makedirs(athlete_folder)\n",
    "\n",
    "    # Save each article into a separate text file\n",
    "    for idx, article in enumerate(articles, start=1):\n",
    "        filename = os.path.join(athlete_folder, f\"article_{idx}.txt\")\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(f\"Title: {article['title']}\\n\")\n",
    "            file.write(f\"URL: {article['url']}\\n\")\n",
    "            file.write(\"Content:\\n\")\n",
    "            file.write(article['content'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of athletes\n",
    "    athletes = [\n",
    "        \"Yomif KEJELCHA\", \"Hagos GEBRHIWET\", \"Berihu AREGAWI\", \"Telahun Haile BEKELE\",\n",
    "        \"Jakob INGEBRIGTSEN\", \"Jacob KIPLIMO\", \"Selemon BAREGA\", \"Grant FISHER\",\n",
    "        \"Luis GRIJALVA\", \"Joshua CHEPTEGEI\"\n",
    "    ]\n",
    "\n",
    "    # Number of articles to fetch for each athlete\n",
    "    num_articles_per_athlete = 10\n",
    "\n",
    "    for athlete in athletes:\n",
    "        print(f\"Fetching articles for: {athlete}\")\n",
    "        articles = fetch_articles_for_athlete(athlete, num_results=num_articles_per_athlete)\n",
    "        save_articles_for_athlete(athlete, articles, \"google-api-data\")\n",
    "        print(f\"Articles for {athlete} saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c619590-542c-4960-91cc-333aaa7ae2a5",
   "metadata": {},
   "source": [
    "**From ESPN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dff494f-81db-40ee-80ed-13b065f2e537",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles for: Yomif KEJELCHA\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://www.google.com/sorry/index?continue=https://www.google.com/search%3Fq%3DYomif%252BKEJELCHA%252Bsite%25253Aespn.com%26num%3D12%26hl%3Den%26start%3D0%26safe%3Dactive&hl=en&q=EhAqAQ4KBbi-EIB2YOdWsrPtGMHZm7MGIjB-lpaXKa7ZaguvZGEptM8xzgDxQeeGl3HcU6AQim-1o_DDld-D1vh5Az9XxiPgmHEyAXJaAUM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m athlete \u001b[38;5;129;01min\u001b[39;00m athletes:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching articles for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mathlete\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m     articles \u001b[38;5;241m=\u001b[39m fetch_articles_for_athlete_from_espn(athlete, num_results\u001b[38;5;241m=\u001b[39mnum_articles_per_athlete)\n\u001b[1;32m     57\u001b[0m     save_articles_for_athlete(athlete, articles, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle-api-data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArticles for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mathlete\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m, in \u001b[0;36mfetch_articles_for_athlete_from_espn\u001b[0;34m(athlete_name, num_results)\u001b[0m\n\u001b[1;32m      8\u001b[0m search_results \u001b[38;5;241m=\u001b[39m search(query, num_results\u001b[38;5;241m=\u001b[39mnum_results, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m articles \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m search_results:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# Make a request to the URL\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/googlesearch/__init__.py:57\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(term, num_results, lang, proxy, advanced, sleep_interval, timeout, safe, ssl_verify)\u001b[0m\n\u001b[1;32m     54\u001b[0m start \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m start \u001b[38;5;241m<\u001b[39m num_results:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Send request\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     resp \u001b[38;5;241m=\u001b[39m _req(escaped_term, num_results \u001b[38;5;241m-\u001b[39m start,\n\u001b[1;32m     58\u001b[0m                 lang, start, proxies, timeout, safe, ssl_verify)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Parse\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(resp\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/googlesearch/__init__.py:26\u001b[0m, in \u001b[0;36m_req\u001b[0;34m(term, results, lang, start, proxies, timeout, safe, ssl_verify)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_req\u001b[39m(term, results, lang, start, proxies, timeout, safe, ssl_verify):\n\u001b[1;32m     10\u001b[0m     resp \u001b[38;5;241m=\u001b[39m get(\n\u001b[1;32m     11\u001b[0m         url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.google.com/search\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m         headers\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m         verify\u001b[38;5;241m=\u001b[39mssl_verify,\n\u001b[1;32m     25\u001b[0m     )\n\u001b[0;32m---> 26\u001b[0m     resp\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://www.google.com/sorry/index?continue=https://www.google.com/search%3Fq%3DYomif%252BKEJELCHA%252Bsite%25253Aespn.com%26num%3D12%26hl%3Den%26start%3D0%26safe%3Dactive&hl=en&q=EhAqAQ4KBbi-EIB2YOdWsrPtGMHZm7MGIjB-lpaXKa7ZaguvZGEptM8xzgDxQeeGl3HcU6AQim-1o_DDld-D1vh5Az9XxiPgmHEyAXJaAUM"
     ]
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def fetch_articles_for_athlete_from_espn(athlete_name, num_results=10):\n",
    "    query = f\"{athlete_name} site:espn.com\"\n",
    "    search_results = search(query, num_results=num_results, lang=\"en\")\n",
    "\n",
    "    articles = []\n",
    "\n",
    "    for url in search_results:\n",
    "        try:\n",
    "            # Make a request to the URL\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract the title\n",
    "            title = soup.find('title').text\n",
    "\n",
    "            # Extract the content\n",
    "            content = ''\n",
    "            for paragraph in soup.find_all('p'):\n",
    "                content += paragraph.text + '\\n'\n",
    "\n",
    "            # Save the article\n",
    "            articles.append({\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'content': content\n",
    "            })\n",
    "\n",
    "        except requests.HTTPError as e:\n",
    "            print(f\"Failed to fetch {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "    return articles\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of athletes\n",
    "    athletes = [\n",
    "        \"Yomif KEJELCHA\", \"Hagos GEBRHIWET\", \"Berihu AREGAWI\", \"Telahun Haile BEKELE\",\n",
    "        \"Jakob INGEBRIGTSEN\", \"Jacob KIPLIMO\", \"Selemon BAREGA\", \"Grant FISHER\",\n",
    "        \"Luis GRIJALVA\", \"Joshua CHEPTEGEI\"\n",
    "    ]\n",
    "\n",
    "    # Number of articles to fetch for each athlete\n",
    "    num_articles_per_athlete = 10\n",
    "\n",
    "    for athlete in athletes:\n",
    "        print(f\"Fetching articles for: {athlete}\")\n",
    "        articles = fetch_articles_for_athlete_from_espn(athlete, num_results=num_articles_per_athlete)\n",
    "        save_articles_for_athlete(athlete, articles, \"google-api-data\")\n",
    "        print(f\"Articles for {athlete} saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1349a577-51a1-48b4-ab31-5b8b6919f253",
   "metadata": {},
   "source": [
    "# Prompt engineering Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bcc7103-4c44-4c5f-9dc0-2e4b9fe46d87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/agathevianeyliaud/opt/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/agathevianeyliaud/opt/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/agathevianeyliaud/opt/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.5.15-cp311-cp311-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/agathevianeyliaud/opt/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp311-cp311-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/agathevianeyliaud/opt/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/agathevianeyliaud/opt/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/agathevianeyliaud/opt/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/agathevianeyliaud/opt/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/agathevianeyliaud/opt/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/agathevianeyliaud/opt/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/agathevianeyliaud/opt/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Using cached transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "Using cached huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "Downloading regex-2024.5.15-cp311-cp311-macosx_10_9_x86_64.whl (281 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.7/281.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp311-cp311-macosx_10_12_x86_64.whl (415 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.3/415.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-macosx_10_12_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.14.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: safetensors, regex, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.14.0 huggingface-hub-0.23.3 regex-2024.5.15 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.41.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389a7939-1c70-4290-a7db-d18a1b455eef",
   "metadata": {},
   "source": [
    "In order to run this code, I need to use a virtual environment (where ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20bfb4f1-524c-4186-bc6e-2e6e097eb5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agathevianeyliaud/opt/anaconda3/envs/tensorflow/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-10 15:18:00.390780: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/Users/agathevianeyliaud/opt/anaconda3/envs/tensorflow/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: Article 1 text...\n",
      "Predicted Label: POSITIVE\n",
      "Confidence: 0.9621729850769043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the correct label (or skip):  Positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: Article 3 text...\n",
      "Predicted Label: POSITIVE\n",
      "Confidence: 0.9514207243919373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the correct label (or skip):  Positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: Article 2 text...\n",
      "Predicted Label: POSITIVE\n",
      "Confidence: 0.9514207243919373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the correct label (or skip):  Negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final labeled dataset:\n",
      "Article: Article 1 text...\n",
      "Label: Positive\n",
      "Article: Article 3 text...\n",
      "Label: Positive\n",
      "Article: Article 2 text...\n",
      "Label: Negative\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load pre-trained text classification model\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\")\n",
    "\n",
    "# Step 2: Define your articles\n",
    "articles = [\n",
    "    \"Article 1 text...\",\n",
    "    \"Article 2 text...\",\n",
    "    \"Article 3 text...\",\n",
    "    # Add more articles as needed\n",
    "]\n",
    "\n",
    "# Step 3: Active learning loop\n",
    "labeled_data = []\n",
    "unlabeled_data = articles.copy()\n",
    "\n",
    "while len(unlabeled_data) > 0:\n",
    "    # Step 3a: Predict labels for unlabeled data\n",
    "    predictions = classifier(unlabeled_data)\n",
    "    \n",
    "    # Step 3b: Select a subset of predictions for manual verification\n",
    "    for article, prediction in zip(unlabeled_data, predictions):\n",
    "        print(\"Article:\", article)\n",
    "        print(\"Predicted Label:\", prediction['label'])\n",
    "        print(\"Confidence:\", prediction['score'])\n",
    "        user_input = input(\"Enter the correct label (or skip): \")\n",
    "        \n",
    "        if user_input.lower() != \"skip\":\n",
    "            labeled_data.append((article, user_input))\n",
    "            unlabeled_data.remove(article)\n",
    "    \n",
    "    # Step 3c: Retrain the model on the expanded labeled dataset\n",
    "    labeled_texts = [article for article, label in labeled_data]\n",
    "    labels = [label for article, label in labeled_data]\n",
    "    \n",
    "    # Perform model retraining with labeled data (code not provided)\n",
    "    # retrain_model(labeled_texts, labels)\n",
    "\n",
    "# Final labeled dataset\n",
    "print(\"Final labeled dataset:\")\n",
    "for article, label in labeled_data:\n",
    "    print(\"Article:\", article)\n",
    "    print(\"Label:\", label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
