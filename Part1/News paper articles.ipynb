{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NewsAPI.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install newsapi-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import pandas as pd\n",
    "\n",
    "# Init\n",
    "with open(\"NewsAPI_key.json\") as infile:\n",
    "    json_obj = json.load(infile)\n",
    "    key =json_obj[\"key\"]\n",
    "    newsapi = NewsApiClient(api_key=key)\n",
    "\n",
    "# Dummy list to store all articles\n",
    "all_articles_list = []\n",
    "\n",
    "# Loop through each word and fetch articles\n",
    "for athlete in athletes:\n",
    "    for page in range(1, 6):  # Loop though pages\n",
    "        articles = newsapi.get_everything(q=athlete,\n",
    "                                          sort_by='relevancy',\n",
    "                                          page=page)\n",
    "        # Extract the articles and add a new column for the search term\n",
    "        if 'articles' in articles:\n",
    "            for article in articles['articles']:\n",
    "                article['athletes'] = athlete\n",
    "                all_articles_list.append(article)\n",
    "        else:\n",
    "            break  # Exit the loop if no more articles are returned\n",
    "\n",
    "# Convert the list of articles into a DataFrame\n",
    "df = pd.DataFrame(all_articles_list)\n",
    "\n",
    "encoded = json.dumps(all_articles_list)\n",
    "all_articles_list = json.loads(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google News API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#401 =not found\n",
    "import os\n",
    "import urllib.parse\n",
    "import requests\n",
    "\n",
    "# Replace with your actual API key\n",
    "API_KEY = os.environ.get(\"API_KEY\")\n",
    "# Set the API key in headers\n",
    "headers = {\"apikey\": API_KEY}\n",
    "\n",
    "# Loop through each athlete and fetch news articles\n",
    "for athlete in athletes:\n",
    "    # Format the query\n",
    "    query = {\"q\": athlete}\n",
    "    \n",
    "    # Build the URL to make the request\n",
    "    url = f\"https://api.serply.io/v1/news?\" + urllib.parse.urlencode(query)\n",
    "    \n",
    "    # Make the request\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if resp.status_code == 200:\n",
    "        results = resp.json()\n",
    "        \n",
    "        # Print the results\n",
    "        print(f\"Results for {athlete}:\")\n",
    "        for article in results.get('articles', []):\n",
    "            print(\"Title:\", article['title'])\n",
    "            print(\"Description:\", article['description'])\n",
    "            print(\"URL:\", article['url'])\n",
    "            print()\n",
    "    else:\n",
    "        print(f\"Failed to fetch results for {athlete}: {resp.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Articles Database json Agathe\n",
    "\n",
    "I didn't know what the articles were about, it seems that it is about the NFL... I still need to perform some more analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the path to the JSON file\n",
    "file_path = 'data-articles/news_0000002.json'\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extracting the necessary details from the JSON data\n",
    "article_text = data[\"text\"]\n",
    "\n",
    "# Print the extracted details\n",
    "print(\"Article Text:\", article_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# Loop through files from 1 to 100\n",
    "for i in range(1, 101):\n",
    "    # Generate the file name with leading zeros\n",
    "    file_path = f'data-articles/news_{i:07d}.json'\n",
    "    \n",
    "    try:\n",
    "        # Open and read the JSON file\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Extracting the necessary details from the JSON data\n",
    "        article_text = data[\"text\"]\n",
    "        \n",
    "        # Print the extracted details\n",
    "        print(f\"Article {i} Text:\", article_text)\n",
    "        print(\"------------------------------------------------------------\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON in file {file_path}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what sports these articles are about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Highlight the sport of each article - put it it the same of the json file\n",
    "\n",
    "# Define the sports keywords to look for\n",
    "sports_keywords = [\"athletic\", \"NFL\", \"baseball\"]\n",
    "\n",
    "# Directory containing the JSON files\n",
    "input_directory_path = 'data-articles'\n",
    "\n",
    "# Directory to save the filtered articles\n",
    "output_directory_path = 'data-articles/filtered-articles'\n",
    "os.makedirs(output_directory_path, exist_ok=True)\n",
    "\n",
    "# Loop through files from 1 to 100\n",
    "for i in range(1, 101):\n",
    "    # Generate the input file name with leading zeros\n",
    "    input_file_path = os.path.join(input_directory_path, f'news_{i:07d}.json')\n",
    "    \n",
    "    try:\n",
    "        # Open and read the JSON file\n",
    "        with open(input_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Extract the text of the article\n",
    "        article_text = data.get(\"text\", \"\")\n",
    "        \n",
    "        # Check if any of the sports keywords are in the article text\n",
    "        found_keywords = [keyword for keyword in sports_keywords if keyword.lower() in article_text.lower()]\n",
    "        if found_keywords:\n",
    "            # Print the found keywords\n",
    "            print(f\"Article {i} mentions the following sports keywords: {', '.join(found_keywords)}\")\n",
    "            \n",
    "            # Save only the text to a new JSON file in the output directory\n",
    "            output_file_path = os.path.join(output_directory_path, f'filtered_{i:07d}_{found_keywords[0]}.json')\n",
    "            with open(output_file_path, 'w') as output_file:\n",
    "                json.dump({\"text\": article_text}, output_file, indent=4)\n",
    "            print(f\"Article {i} text has been saved to {output_file_path}.\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {input_file_path} not found.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON in file {input_file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you can see, we have mostly baseball and NFL articles....\n",
    "\n",
    "Let's find something else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get articles from google - Agathe\n",
    "Webscraping (not API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install googlesearch-python requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Find articles from a google search - RANDOM SPORTS article for nom\n",
    "\n",
    "def fetch_wikipedia_articles(query, num_results=10):\n",
    "    # Perform Google search limited to Wikipedia\n",
    "    search_results = search(query + \" site:en.wikipedia.org\", num_results=num_results, lang=\"en\")\n",
    "    \n",
    "    articles = []\n",
    "\n",
    "    for url in search_results:\n",
    "        try:\n",
    "            # Make a request to the URL\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extract the title\n",
    "            title = soup.find('title').text\n",
    "            \n",
    "            # Extract the content\n",
    "            content = ''\n",
    "            for paragraph in soup.find_all('p'):\n",
    "                content += paragraph.text + '\\n'\n",
    "            \n",
    "            # Save the article\n",
    "            articles.append({\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'content': content\n",
    "            })\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to fetch {url}: {e}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def save_articles(articles, folder_name):\n",
    "    # Create folder if it doesn't exist\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    \n",
    "    for idx, article in enumerate(articles, start=1):\n",
    "        filename = os.path.join(folder_name, f\"article_{idx}.txt\")\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(f\"Title: {article['title']}\\n\")\n",
    "            file.write(f\"URL: {article['url']}\\n\")\n",
    "            file.write(\"Content:\\n\")\n",
    "            file.write(article['content'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example queries\n",
    "    queries = [\"athletics performance\", \"gymnastics\"]\n",
    "    \n",
    "    all_articles = {}\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"Fetching Wikipedia articles for: {query}\")\n",
    "        articles = fetch_wikipedia_articles(query)\n",
    "        all_articles[query] = articles\n",
    "        \n",
    "    for query, articles in all_articles.items():\n",
    "        print(f\"\\nWikipedia Articles for query: {query}\")\n",
    "        save_articles(articles, \"google-api-data\")\n",
    "        print(\"Articles saved.\")\n",
    "\n",
    "### BE CAREFUL\n",
    "# You can get an error if you run too often this cell!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to get the articles of athletes. Let's first look at the 5000m 10 best athletes.\n",
    "Yomif KEJELCHA, Hagos GEBRHIWET, Berihu AREGAWI, Telahun Haile BEKELE, Jakob INGEBRIGTSEN, Jacob KIPLIMO, Selemon BAREGA,Grant FISHER,Luis GRIJALVA,Joshua CHEPTEGEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def fetch_articles_for_athlete(athlete_name, num_results=10):\n",
    "    query = f\"{athlete_name} 5000m athlete\"\n",
    "    search_results = search(query, num_results=num_results, lang=\"en\")\n",
    "\n",
    "    articles = []\n",
    "\n",
    "    for url in search_results:\n",
    "        try:\n",
    "            # Make a request to the URL\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract the title\n",
    "            title = soup.find('title').text\n",
    "\n",
    "            # Extract the content\n",
    "            content = ''\n",
    "            for paragraph in soup.find_all('p'):\n",
    "                content += paragraph.text + '\\n'\n",
    "\n",
    "            # Save the article\n",
    "            articles.append({\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'content': content\n",
    "            })\n",
    "\n",
    "        except requests.HTTPError as e:\n",
    "            print(f\"Failed to fetch {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "    return articles\n",
    "\n",
    "def save_articles_for_athlete(athlete_name, articles, folder_name):\n",
    "    # Create a folder for the athlete if it doesn't exist\n",
    "    athlete_folder = os.path.join(folder_name, athlete_name.replace(\" \", \"_\"))\n",
    "    if not os.path.exists(athlete_folder):\n",
    "        os.makedirs(athlete_folder)\n",
    "\n",
    "    # Save each article into a separate text file\n",
    "    for idx, article in enumerate(articles, start=1):\n",
    "        filename = os.path.join(athlete_folder, f\"article_{idx}.txt\")\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(f\"Title: {article['title']}\\n\")\n",
    "            file.write(f\"URL: {article['url']}\\n\")\n",
    "            file.write(\"Content:\\n\")\n",
    "            file.write(article['content'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of athletes\n",
    "    athletes = [\n",
    "        \"Yomif KEJELCHA\", \"Hagos GEBRHIWET\", \"Berihu AREGAWI\", \"Telahun Haile BEKELE\",\n",
    "        \"Jakob INGEBRIGTSEN\", \"Jacob KIPLIMO\", \"Selemon BAREGA\", \"Grant FISHER\",\n",
    "        \"Luis GRIJALVA\", \"Joshua CHEPTEGEI\"\n",
    "    ]\n",
    "\n",
    "    # Number of articles to fetch for each athlete\n",
    "    num_articles_per_athlete = 10\n",
    "\n",
    "    for athlete in athletes:\n",
    "        print(f\"Fetching articles for: {athlete}\")\n",
    "        articles = fetch_articles_for_athlete(athlete, num_results=num_articles_per_athlete)\n",
    "        save_articles_for_athlete(athlete, articles, \"google-api-data\")\n",
    "        print(f\"Articles for {athlete} saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From ESPN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def fetch_articles_for_athlete_from_espn(athlete_name, num_results=10):\n",
    "    query = f\"{athlete_name} site:espn.com\"\n",
    "    search_results = search(query, num_results=num_results, lang=\"en\")\n",
    "\n",
    "    articles = []\n",
    "\n",
    "    for url in search_results:\n",
    "        try:\n",
    "            # Make a request to the URL\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract the title\n",
    "            title = soup.find('title').text\n",
    "\n",
    "            # Extract the content\n",
    "            content = ''\n",
    "            for paragraph in soup.find_all('p'):\n",
    "                content += paragraph.text + '\\n'\n",
    "\n",
    "            # Save the article\n",
    "            articles.append({\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'content': content\n",
    "            })\n",
    "\n",
    "        except requests.HTTPError as e:\n",
    "            print(f\"Failed to fetch {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "    return articles\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of athletes\n",
    "    athletes = [\n",
    "        \"Yomif KEJELCHA\", \"Hagos GEBRHIWET\", \"Berihu AREGAWI\", \"Telahun Haile BEKELE\",\n",
    "        \"Jakob INGEBRIGTSEN\", \"Jacob KIPLIMO\", \"Selemon BAREGA\", \"Grant FISHER\",\n",
    "        \"Luis GRIJALVA\", \"Joshua CHEPTEGEI\"\n",
    "    ]\n",
    "\n",
    "    # Number of articles to fetch for each athlete\n",
    "    num_articles_per_athlete = 10\n",
    "\n",
    "    for athlete in athletes:\n",
    "        print(f\"Fetching articles for: {athlete}\")\n",
    "        articles = fetch_articles_for_athlete_from_espn(athlete, num_results=num_articles_per_athlete)\n",
    "        save_articles_for_athlete(athlete, articles, \"google-api-data\")\n",
    "        print(f\"Articles for {athlete} saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt engineering Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this code, I need to use a virtual environment (where ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Load pre-trained text classification model\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\")\n",
    "\n",
    "# Step 2: Define your articles\n",
    "articles = [\n",
    "    \"Article 1 text...\",\n",
    "    \"Article 2 text...\",\n",
    "    \"Article 3 text...\",\n",
    "    # Add more articles as needed\n",
    "]\n",
    "\n",
    "# Step 3: Active learning loop\n",
    "labeled_data = []\n",
    "unlabeled_data = articles.copy()\n",
    "\n",
    "while len(unlabeled_data) > 0:\n",
    "    # Step 3a: Predict labels for unlabeled data\n",
    "    predictions = classifier(unlabeled_data)\n",
    "    \n",
    "    # Step 3b: Select a subset of predictions for manual verification\n",
    "    for article, prediction in zip(unlabeled_data, predictions):\n",
    "        print(\"Article:\", article)\n",
    "        print(\"Predicted Label:\", prediction['label'])\n",
    "        print(\"Confidence:\", prediction['score'])\n",
    "        user_input = input(\"Enter the correct label (or skip): \")\n",
    "        \n",
    "        if user_input.lower() != \"skip\":\n",
    "            labeled_data.append((article, user_input))\n",
    "            unlabeled_data.remove(article)\n",
    "    \n",
    "    # Step 3c: Retrain the model on the expanded labeled dataset\n",
    "    labeled_texts = [article for article, label in labeled_data]\n",
    "    labels = [label for article, label in labeled_data]\n",
    "    \n",
    "    # Perform model retraining with labeled data (code not provided)\n",
    "    # retrain_model(labeled_texts, labels)\n",
    "\n",
    "# Final labeled dataset\n",
    "print(\"Final labeled dataset:\")\n",
    "for article, label in labeled_data:\n",
    "    print(\"Article:\", article)\n",
    "    print(\"Label:\", label)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
